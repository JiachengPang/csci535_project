{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from transformers import HubertModel, Wav2Vec2FeatureExtractor, RobertaTokenizer, RobertaModel\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "import numpy\n",
    "import librosa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['file', 'audio', 'frustrated', 'angry', 'sad', 'disgust', 'excited', 'fear', 'neutral', 'surprise', 'happy', 'EmoAct', 'EmoVal', 'EmoDom', 'gender', 'transcription', 'major_emotion', 'speaking_rate', 'pitch_mean', 'pitch_std', 'rms', 'relative_db'],\n",
       "        num_rows: 10039\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iemocap = load_from_disk('./iemocap')\n",
    "iemocap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_labels = ['neutral', 'happy', 'sad', 'angry', 'frustrated', 'excited', 'fear', 'disgust', 'surprise']\n",
    "label_to_idx = {label: idx for idx, label in enumerate(emotion_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': 'Ses01F_impro01_F000.wav',\n",
       " 'audio': {'path': 'Ses01F_impro01_F000.wav',\n",
       "  'array': array([-0.0050354 , -0.00497437, -0.0038147 , ..., -0.00265503,\n",
       "         -0.00317383, -0.00418091]),\n",
       "  'sampling_rate': 16000},\n",
       " 'frustrated': 0.0062500000931322575,\n",
       " 'angry': 0.0062500000931322575,\n",
       " 'sad': 0.0062500000931322575,\n",
       " 'disgust': 0.0062500000931322575,\n",
       " 'excited': 0.0062500000931322575,\n",
       " 'fear': 0.0062500000931322575,\n",
       " 'neutral': 0.949999988079071,\n",
       " 'surprise': 0.0062500000931322575,\n",
       " 'happy': 0.0062500000931322575,\n",
       " 'EmoAct': 2.3333330154418945,\n",
       " 'EmoVal': 2.6666669845581055,\n",
       " 'EmoDom': 2.0,\n",
       " 'gender': 'Female',\n",
       " 'transcription': ' Excuse me.',\n",
       " 'major_emotion': 'neutral',\n",
       " 'speaking_rate': 5.139999866485596,\n",
       " 'pitch_mean': 202.79881286621094,\n",
       " 'pitch_std': 76.12785339355469,\n",
       " 'rms': 0.00788376946002245,\n",
       " 'relative_db': -17.938434600830078}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = iemocap['train'][0]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "auido_checkpooint = \"facebook/hubert-base-ls960\"\n",
    "text_checkpoint = 'roberta-base\"\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(auido_checkpooint)\n",
    "hubert_model = HubertModel.from_pretrained(auido_checkpooint)\n",
    "\n",
    "roberta_model = RobertaModel.from_pretrained(text_checkpoint)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(text_checkpoint)\n",
    "hubert_model.eval()\n",
    "roberta_model.eval()\n",
    "\n",
    "# Extract HuBERT features\n",
    "def extract_hubert_features(audio_array, sampling_rate=16000):\n",
    "    # audio_tensor = torch.tensor(audio_array).unsqueeze(0)\n",
    "    audio_tensor = torch.tensor(audio_array)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        input_values = feature_extractor(audio_tensor, return_tensors=\"pt\", sampling_rate=sampling_rate).input_values\n",
    "        hubert_features = hubert_model(input_values).last_hidden_state\n",
    "\n",
    "    # print(f'hubert_features: {hubert_features.shape}')    \n",
    "    return torch.mean(hubert_features, dim=1)  # avg pooling\n",
    "  \n",
    "# Extract RoBERTa features\n",
    "def extract_roberta_features(transcript):\n",
    "    tokens = tokenizer(transcript, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        roberta_output = roberta_model(**tokens)\n",
    "    \n",
    "    # print(f'roberta_output: {roberta_output.last_hidden_state[:, 0, :].shape}')\n",
    "    return roberta_output.last_hidden_state[:, 0, :]  # CLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, audio_dim=768, text_dim=768, hidden_dim=512, num_classes=9):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(audio_dim + text_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, audio_emb, text_emb):\n",
    "        x = torch.cat((audio_emb, text_emb), dim=1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "classifier = Classifier()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.0005)\n",
    "\n",
    "def train_step(audio_array, transcript, label):\n",
    "    classifier.train()\n",
    "\n",
    "    audio_emb = extract_hubert_features(audio_array)\n",
    "    # print(f'audio_emb: {audio_emb.shape}')\n",
    "    text_emb = extract_roberta_features(transcript)\n",
    "    # print(f'text_emb: {text_emb.shape}')\n",
    "\n",
    "    output = classifier(audio_emb, text_emb)\n",
    "    # print(f'output: {output.shape}')\n",
    "\n",
    "    target = torch.tensor([label]).long()\n",
    "    loss = criterion(output, target)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/200, Loss: 2.2045\n",
      "Step 10/200, Loss: 1.9784\n",
      "Step 20/200, Loss: 0.8544\n",
      "Step 30/200, Loss: 5.3096\n",
      "Step 40/200, Loss: 2.8518\n",
      "Step 50/200, Loss: 1.1458\n",
      "Step 60/200, Loss: 0.3454\n",
      "Step 70/200, Loss: 4.1522\n",
      "Step 80/200, Loss: 2.7448\n",
      "Step 90/200, Loss: 1.8373\n",
      "Step 100/200, Loss: 2.2920\n",
      "Step 110/200, Loss: 1.4440\n",
      "Step 120/200, Loss: 1.4263\n",
      "Step 130/200, Loss: 1.1908\n",
      "Step 140/200, Loss: 3.0186\n",
      "Step 150/200, Loss: 2.8815\n",
      "Step 160/200, Loss: 1.0167\n",
      "Step 170/200, Loss: 0.7201\n",
      "Step 180/200, Loss: 0.2134\n",
      "Step 190/200, Loss: 2.9178\n"
     ]
    }
   ],
   "source": [
    "num_samples = 200\n",
    "\n",
    "for i in range(num_samples):\n",
    "    sample = iemocap['train'][i]\n",
    "    \n",
    "    # Extract audio and transcript\n",
    "    audio_array = sample['audio']['array']\n",
    "    transcript = sample['transcription']\n",
    "    \n",
    "    \n",
    "    # Get emotion label\n",
    "    label = label_to_idx[sample['major_emotion']]\n",
    "\n",
    "    # Train\n",
    "    loss = train_step(audio_array, transcript, label)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(f'Step {i}/{num_samples}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 100, correct = 49, acc = 0.49\n"
     ]
    }
   ],
   "source": [
    "def predict(audio_array, transcript):\n",
    "    classifier.eval()\n",
    "    \n",
    "    # Extract features\n",
    "    audio_embedding = extract_hubert_features(audio_array)\n",
    "    text_embedding = extract_roberta_features(transcript)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = classifier(audio_embedding, text_embedding)\n",
    "        predicted_label = torch.argmax(F.softmax(output, dim=1), dim=1).item()\n",
    "\n",
    "    return emotion_labels[predicted_label]\n",
    "\n",
    "correct = 0\n",
    "n_samples = 100\n",
    "for i in range(1000, 1000 + n_samples, 1):\n",
    "    sample = iemocap['train'][i]\n",
    "    prediction = predict(sample['audio']['array'], sample['transcription'])\n",
    "    if prediction == sample['major_emotion']:\n",
    "        correct += 1\n",
    "\n",
    "print(f'total: {n_samples}, correct = {correct}, acc = {correct / n_samples}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
