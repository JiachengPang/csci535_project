{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from transformers import HubertModel, Wav2Vec2FeatureExtractor, RobertaTokenizer, RobertaModel\n",
    "\n",
    "save_dir = \"precomputed_embeddings\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "audio_checkpoint = 'facebook/hubert-base-ls960'\n",
    "text_checkpoint = 'roberta-base'\n",
    "\n",
    "hubert_model = HubertModel.from_pretrained(audio_checkpoint).to(device)\n",
    "hubert_processor = Wav2Vec2FeatureExtractor.from_pretrained(audio_checkpoint)\n",
    "\n",
    "roberta_model = RobertaModel.from_pretrained(text_checkpoint).to(device)\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained(text_checkpoint)\n",
    "\n",
    "\n",
    "def extract_hubert_features(audio_array):\n",
    "    audio_tensor = torch.tensor(audio_array).to(device)\n",
    "    input_values = hubert_processor(audio_tensor, return_tensors=\"pt\", sampling_rate=16000).input_values.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hubert_features = hubert_model(input_values).last_hidden_state  # (1, seq_len, 768)\n",
    "\n",
    "    return torch.mean(hubert_features, dim=1).squeeze(0).tolist()\n",
    "\n",
    "\n",
    "def extract_roberta_features(transcript):\n",
    "    tokens = roberta_tokenizer(transcript, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    tokens = {key: val.to(device) for key, val in tokens.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        roberta_output = roberta_model(**tokens)\n",
    "\n",
    "    return roberta_output.last_hidden_state[:, 0, :].squeeze(0).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['file', 'audio', 'frustrated', 'angry', 'sad', 'disgust', 'excited', 'fear', 'neutral', 'surprise', 'happy', 'EmoAct', 'EmoVal', 'EmoDom', 'gender', 'transcription', 'major_emotion', 'speaking_rate', 'pitch_mean', 'pitch_std', 'rms', 'relative_db'],\n",
       "        num_rows: 10039\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_from_disk('./iemocap')\n",
    "\n",
    "emotion_labels = ['neutral', 'happy', 'sad', 'angry', 'frustrated', 'excited', 'fear', 'disgust', 'surprise', 'other']\n",
    "num_classes = len(emotion_labels)\n",
    "\n",
    "label_to_idx = {label: idx for idx, label in enumerate(emotion_labels)}\n",
    "\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HuBERT Model Device: cuda:0\n",
      "RoBERTa Model Device: cuda:0\n",
      "Expected Device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(f\"HuBERT Model Device: {next(hubert_model.parameters()).device}\")\n",
    "print(f\"RoBERTa Model Device: {next(roberta_model.parameters()).device}\")\n",
    "print(f\"Expected Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10039/10039 [17:23<00:00,  9.62 examples/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting embeddings\")\n",
    "\n",
    "modified_dataset = dataset.map(\n",
    "    lambda example: {\n",
    "        \"audio_embedding\": extract_hubert_features(example[\"audio\"][\"array\"]),\n",
    "        \"text_embedding\": extract_roberta_features(example[\"transcription\"]),\n",
    "        \"label_id\": label_to_idx[example[\"major_emotion\"]]\n",
    "    },\n",
    "    batched=False,\n",
    "    load_from_cache_file=False\n",
    ")\n",
    "\n",
    "print(\"Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (4/4 shards): 100%|██████████| 10039/10039 [01:39<00:00, 101.09 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with embeddings saved to ./iemocap_precomputed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "save_path = \"./iemocap_precomputed\"\n",
    "modified_dataset.save_to_disk(save_path)\n",
    "print(f\"Dataset with embeddings saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['file', 'audio', 'frustrated', 'angry', 'sad', 'disgust', 'excited', 'fear', 'neutral', 'surprise', 'happy', 'EmoAct', 'EmoVal', 'EmoDom', 'gender', 'transcription', 'major_emotion', 'speaking_rate', 'pitch_mean', 'pitch_std', 'rms', 'relative_db', 'audio_embedding', 'text_embedding', 'label_id'],\n",
       "        num_rows: 10039\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "535ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
