{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from transformers import HubertModel, Wav2Vec2FeatureExtractor, RobertaTokenizer, RobertaModel\n",
    "\n",
    "save_dir = \"precomputed_embeddings\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "audio_checkpoint = 'facebook/hubert-base-ls960'\n",
    "text_checkpoint = 'roberta-base'\n",
    "\n",
    "hubert_model = HubertModel.from_pretrained(audio_checkpoint).to(device)\n",
    "hubert_processor = Wav2Vec2FeatureExtractor.from_pretrained(audio_checkpoint)\n",
    "\n",
    "roberta_model = RobertaModel.from_pretrained(text_checkpoint).to(device)\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained(text_checkpoint)\n",
    "\n",
    "\n",
    "def extract_hubert_features(audio_array):\n",
    "    audio_tensor = torch.tensor(audio_array).to(device)\n",
    "    input_values = hubert_processor(audio_tensor, return_tensors=\"pt\", sampling_rate=16000).input_values.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hubert_features = hubert_model(input_values).last_hidden_state  # (1, seq_len, 768)\n",
    "\n",
    "    return torch.mean(hubert_features, dim=1).squeeze(0).tolist()\n",
    "\n",
    "\n",
    "def extract_roberta_features(transcript):\n",
    "    tokens = roberta_tokenizer(transcript, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    tokens = {key: val.to(device) for key, val in tokens.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        roberta_output = roberta_model(**tokens)\n",
    "\n",
    "    return roberta_output.last_hidden_state[:, 0, :].squeeze(0).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['file', 'audio', 'frustrated', 'angry', 'sad', 'disgust', 'excited', 'fear', 'neutral', 'surprise', 'happy', 'EmoAct', 'EmoVal', 'EmoDom', 'gender', 'transcription', 'major_emotion', 'speaking_rate', 'pitch_mean', 'pitch_std', 'rms', 'relative_db'],\n",
       "        num_rows: 10039\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_from_disk('./iemocap')\n",
    "\n",
    "emotion_labels = ['neutral', 'happy', 'sad', 'angry', 'frustrated', 'excited', 'fear', 'disgust', 'surprise', 'other']\n",
    "num_classes = len(emotion_labels)\n",
    "\n",
    "label_to_idx = {label: idx for idx, label in enumerate(emotion_labels)}\n",
    "\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HuBERT Model Device: cuda:0\n",
      "RoBERTa Model Device: cuda:0\n",
      "Expected Device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(f\"HuBERT Model Device: {next(hubert_model.parameters()).device}\")\n",
    "print(f\"RoBERTa Model Device: {next(roberta_model.parameters()).device}\")\n",
    "print(f\"Expected Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function <lambda> at 0xff58c1628360> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4957697b3bf4c50872fff9a8c9f1d21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10039 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting embeddings\")\n",
    "\n",
    "modified_dataset = dataset.map(\n",
    "    lambda example: {\n",
    "        \"audio_embedding\": extract_hubert_features(example[\"audio\"][\"array\"]),\n",
    "        \"text_embedding\": extract_roberta_features(example[\"transcription\"]),\n",
    "        \"label_id\": label_to_idx[example[\"major_emotion\"]]\n",
    "    },\n",
    "    batched=False,\n",
    "    load_from_cache_file=False\n",
    ")\n",
    "\n",
    "print(\"Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcbc4e219a654f0dbcbb6c321bf643fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/4 shards):   0%|          | 0/10039 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with embeddings saved to ./iemocap_precomputed\n"
     ]
    }
   ],
   "source": [
    "save_path = \"./iemocap_precomputed\"\n",
    "modified_dataset.save_to_disk(save_path)\n",
    "print(f\"Dataset with embeddings saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['file', 'audio', 'frustrated', 'angry', 'sad', 'disgust', 'excited', 'fear', 'neutral', 'surprise', 'happy', 'EmoAct', 'EmoVal', 'EmoDom', 'gender', 'transcription', 'major_emotion', 'speaking_rate', 'pitch_mean', 'pitch_std', 'rms', 'relative_db', 'audio_embedding', 'text_embedding', 'label_id'],\n",
       "        num_rows: 10039\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "535",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
