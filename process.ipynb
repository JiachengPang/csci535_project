{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import base64\n",
    "from datasets import load_from_disk\n",
    "from openai import AsyncOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import asyncio\n",
    "import wave\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "load_dotenv()\n",
    "client = AsyncOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_wav_bytes(audio_array, sample_rate):\n",
    "    \"\"\"\n",
    "    Convert a numpy audio array and sample rate into WAV bytes.\n",
    "    Assumes mono audio. If the array is not int16, it converts assuming \n",
    "    the values are in the range [-1, 1].\n",
    "    \"\"\"\n",
    "    audio_array = np.array(audio_array)\n",
    "    if audio_array.dtype != np.int16:\n",
    "        # Convert normalized float audio to int16\n",
    "        audio_array = np.int16(np.clip(audio_array, -1, 1) * 32767)\n",
    "    \n",
    "    buffer = io.BytesIO()\n",
    "    with wave.open(buffer, 'wb') as wf:\n",
    "        channels = 1  # adjust if needed\n",
    "        wf.setnchannels(channels)\n",
    "        wf.setsampwidth(2)  # int16 is 2 bytes\n",
    "        wf.setframerate(sample_rate)\n",
    "        wf.writeframes(audio_array.tobytes())\n",
    "    wav_bytes = buffer.getvalue()\n",
    "    buffer.close()\n",
    "    return wav_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_sample(sample):\n",
    "    # Use the \"file\" field as a primary key for identification\n",
    "    primary_key = sample[\"file\"]\n",
    "    \n",
    "    audio_info = sample[\"audio\"]\n",
    "    audio_array = audio_info[\"array\"]\n",
    "    sample_rate = audio_info[\"sampling_rate\"]\n",
    "    \n",
    "    # Encode the audio to a base64 string\n",
    "    wav_bytes = array_to_wav_bytes(audio_array, sample_rate)\n",
    "    encoded_audio = base64.b64encode(wav_bytes).decode('utf-8')\n",
    "    \n",
    "    # Create text prompts combining transcript and additional metadata\n",
    "    text_prompt = (\n",
    "        \"Generate a detailed and descriptive emotion caption based solely on the vocal qualities of the following audio recording. \"\n",
    "        \"The caption should capture nuances such as tone, inflection, and speech characteristics without referencing the scene or contextual details. \"\n",
    "        \"The caption should be in the form of a sentence. \"\n",
    "        \"Avoid simply stating an emotion word; instead, describe the vocal expression. For example, 'The voice was vehement, the tone revealing inner dissatisfaction and complaint.'\"\n",
    "    )\n",
    "\n",
    "    details_text = (\n",
    "        f\"Transcript: {sample['transcription']}\\n\"\n",
    "        f\"Pitch: mean {sample['pitch_mean']}, std {sample['pitch_std']}\\n\"\n",
    "        f\"Major Emotion: {sample['major_emotion']}\\n\"\n",
    "        f\"Speaking Rate: {sample['speaking_rate']}\"\n",
    "    )\n",
    "    \n",
    "    # Build the payload for the gpt-4o-audio-preview model\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4o-audio-preview\",\n",
    "        \"modalities\": [\"text\"],\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": text_prompt},\n",
    "                    {\"type\": \"text\", \"text\": details_text},\n",
    "                    {\n",
    "                        \"type\": \"input_audio\",\n",
    "                        \"input_audio\": {\n",
    "                            \"data\": encoded_audio,\n",
    "                            \"format\": \"wav\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Asynchronously call the chat completion endpoint\n",
    "    response = await client.chat.completions.create(**payload)\n",
    "    return primary_key, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file': 'Ses01F_impro01_F000.wav', 'audio': {'path': 'Ses01F_impro01_F000.wav', 'array': array([-0.0050354 , -0.00497437, -0.0038147 , ..., -0.00265503,\n",
      "       -0.00317383, -0.00418091]), 'sampling_rate': 16000}, 'frustrated': 0.0062500000931322575, 'angry': 0.0062500000931322575, 'sad': 0.0062500000931322575, 'disgust': 0.0062500000931322575, 'excited': 0.0062500000931322575, 'fear': 0.0062500000931322575, 'neutral': 0.949999988079071, 'surprise': 0.0062500000931322575, 'happy': 0.0062500000931322575, 'EmoAct': 2.3333330154418945, 'EmoVal': 2.6666669845581055, 'EmoDom': 2.0, 'gender': 'Female', 'transcription': ' Excuse me.', 'major_emotion': 'neutral', 'speaking_rate': 5.139999866485596, 'pitch_mean': 202.79881286621094, 'pitch_std': 76.12785339355469, 'rms': 0.00788376946002245, 'relative_db': -17.938434600830078}\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset from disk\n",
    "ds = load_from_disk(\"iemocap\")\n",
    "train_data = ds[\"train\"]\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file': 'Ses01F_impro01_F000.wav', 'audio': {'path': 'Ses01F_impro01_F000.wav', 'array': array([-0.0050354 , -0.00497437, -0.0038147 , ..., -0.00265503,\n",
      "       -0.00317383, -0.00418091]), 'sampling_rate': 16000}, 'frustrated': 0.0062500000931322575, 'angry': 0.0062500000931322575, 'sad': 0.0062500000931322575, 'disgust': 0.0062500000931322575, 'excited': 0.0062500000931322575, 'fear': 0.0062500000931322575, 'neutral': 0.949999988079071, 'surprise': 0.0062500000931322575, 'happy': 0.0062500000931322575, 'EmoAct': 2.3333330154418945, 'EmoVal': 2.6666669845581055, 'EmoDom': 2.0, 'gender': 'Female', 'transcription': ' Excuse me.', 'major_emotion': 'neutral', 'speaking_rate': 5.139999866485596, 'pitch_mean': 202.79881286621094, 'pitch_std': 76.12785339355469, 'rms': 0.00788376946002245, 'relative_db': -17.938434600830078}\n"
     ]
    }
   ],
   "source": [
    "first_ten_samples = train_data.select(range(10))\n",
    "print(first_ten_samples[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/21 (samples 0 to 499)\n",
      "Batch 1 completed and saved\n",
      "Processing batch 2/21 (samples 500 to 999)\n",
      "Batch 2 completed and saved\n",
      "Processing batch 3/21 (samples 1000 to 1499)\n",
      "Batch 3 completed and saved\n",
      "Processing batch 4/21 (samples 1500 to 1999)\n",
      "Batch 4 completed and saved\n",
      "Processing batch 5/21 (samples 2000 to 2499)\n",
      "Batch 5 completed and saved\n",
      "Processing batch 6/21 (samples 2500 to 2999)\n",
      "Batch 6 completed and saved\n",
      "Processing batch 7/21 (samples 3000 to 3499)\n",
      "Batch 7 completed and saved\n",
      "Processing batch 8/21 (samples 3500 to 3999)\n",
      "Batch 8 completed and saved\n",
      "Processing batch 9/21 (samples 4000 to 4499)\n",
      "Batch 9 completed and saved\n",
      "Processing batch 10/21 (samples 4500 to 4999)\n",
      "Batch 10 completed and saved\n",
      "Processing batch 11/21 (samples 5000 to 5499)\n",
      "Batch 11 completed and saved\n",
      "Processing batch 12/21 (samples 5500 to 5999)\n",
      "Batch 12 completed and saved\n",
      "Processing batch 13/21 (samples 6000 to 6499)\n",
      "Batch 13 completed and saved\n",
      "Processing batch 14/21 (samples 6500 to 6999)\n",
      "Batch 14 completed and saved\n",
      "Processing batch 15/21 (samples 7000 to 7499)\n",
      "Batch 15 completed and saved\n",
      "Processing batch 16/21 (samples 7500 to 7999)\n",
      "Batch 16 completed and saved\n",
      "Processing batch 17/21 (samples 8000 to 8499)\n",
      "Batch 17 completed and saved\n",
      "Processing batch 18/21 (samples 8500 to 8999)\n",
      "Batch 18 completed and saved\n",
      "Processing batch 19/21 (samples 9000 to 9499)\n",
      "Batch 19 completed and saved\n",
      "Processing batch 20/21 (samples 9500 to 9999)\n",
      "Batch 20 completed and saved\n",
      "Processing batch 21/21 (samples 10000 to 10038)\n",
      "Batch 21 completed and saved\n",
      "All results saved to gpt4o_audio_responses.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Process samples in batches of 500\n",
    "batch_size = 500\n",
    "total_samples = len(train_data)\n",
    "num_batches = math.ceil(total_samples / batch_size)\n",
    "\n",
    "results_data = []\n",
    "\n",
    "for batch_idx in range(num_batches):\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = min((batch_idx + 1) * batch_size, total_samples)\n",
    "    \n",
    "    print(f\"Processing batch {batch_idx + 1}/{num_batches} (samples {start_idx} to {end_idx-1})\")\n",
    "    \n",
    "    # Create tasks for this batch only\n",
    "    batch_samples = train_data.select(range(start_idx, end_idx))\n",
    "    batch_tasks = [asyncio.create_task(process_sample(sample)) for sample in batch_samples]\n",
    "    \n",
    "    # Process this batch\n",
    "    batch_results = await asyncio.gather(*batch_tasks)\n",
    "    \n",
    "    # Collect results from this batch\n",
    "    for custom_id, response in batch_results:\n",
    "        content = response.choices[0].message.content\n",
    "        results_data.append({\"id\": custom_id, \"response\": content})\n",
    "    \n",
    "    # Save intermediate results after each batch\n",
    "    interim_df = pd.DataFrame(results_data)\n",
    "    interim_df.to_csv(f\"gpt4o_audio_responses_batch_{batch_idx+1}.csv\", index=False)\n",
    "    print(f\"Batch {batch_idx+1} completed and saved\")\n",
    "\n",
    "# Convert all results to DataFrame and save to final CSV\n",
    "results_df = pd.DataFrame(results_data)\n",
    "results_df.to_csv(\"gpt4o_audio_responses.csv\", index=False)\n",
    "print(f\"All results saved to gpt4o_audio_responses.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
